{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8a4085",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import os\n",
    "import category_encoders as ce\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "255e610c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def load_data(self, file_path: str, output_file_path=None) -> pd.DataFrame:\n",
    "        '''\n",
    "        读取csv或文本文件，支持单个文件或文件夹\n",
    "\n",
    "        file_path: str, 文件路径或文件夹路径\n",
    "        output_file_path: str, 输出文件路径, 如果不为None, 则将数据转为csv保存到该路径\n",
    "        '''\n",
    "        if os.path.isfile(file_path):\n",
    "            # 如果是文件，直接读取\n",
    "            df = pd.read_csv(file_path, header=None)\n",
    "        elif os.path.isdir(file_path):\n",
    "            # 如果是文件夹，读取所有文件并整合\n",
    "            all_files = [os.path.join(file_path, f)\n",
    "                         for f in os.listdir(file_path)]\n",
    "            df_list = [pd.read_csv(f, header=None) for f in all_files]\n",
    "            df = pd.concat(df_list, ignore_index=True)\n",
    "        else:\n",
    "            raise ValueError(f\"{file_path} 不是有效的文件或文件夹路径\")\n",
    "\n",
    "        # 如果不是 CSV 文件，添加列名\n",
    "        if not file_path.endswith('.csv'):\n",
    "            df.columns = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status',\n",
    "                          'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss',\n",
    "                          'hours-per-week', 'native-country', 'income']\n",
    "\n",
    "        # 如果指定了输出路径，保存为 CSV 文件\n",
    "        if output_file_path:\n",
    "            df.to_csv(output_file_path, index=False)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def save_data(self, data, file_path: str):\n",
    "        '''\n",
    "        保存数据到csv文件\n",
    "        data: pd.DataFrame 或 numpy.ndarray, 数据\n",
    "        file_path: str, 文件路径\n",
    "        '''\n",
    "        if isinstance(data, np.ndarray):\n",
    "            data = pd.DataFrame(data)\n",
    "        data.to_csv(file_path, index=False)\n",
    "        return data\n",
    "\n",
    "    def data_noise(self, df: pd.DataFrame, noise_level: float = 0.2) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        增加高斯噪音、随机clip和mask\n",
    "        df: pd.DataFrame, 输入数据\n",
    "        noise_level: float, 噪声强度\n",
    "        return: pd.DataFrame, 添加噪声后的数据\n",
    "        \"\"\"\n",
    "        # 增加高斯噪声\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        for col in numeric_cols:\n",
    "            df[col] += np.random.normal(0, noise_level *\n",
    "                                        df[col].std(), size=df[col].shape)\n",
    "\n",
    "        # 随机clip\n",
    "        for col in numeric_cols:\n",
    "            lower_bound = df[col].quantile(0.01)\n",
    "            upper_bound = df[col].quantile(0.99)\n",
    "            df[col] = df[col].clip(lower=lower_bound, upper=upper_bound)\n",
    "\n",
    "        # # 随机mask\n",
    "        # mask_prob = 0.01  # 1% 的概率将值设置为 NaN\n",
    "        # mask = np.random.rand(*df.shape) < mask_prob\n",
    "        # df = df.mask(mask)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def preprocess_data(self, df: pd.DataFrame,\n",
    "                        missing_value_method: str = \"impute\",\n",
    "                        low_card_method: str = \"onehot\",\n",
    "                        high_card_mehtod: str = \"target\",\n",
    "                        if_standard: bool = True,\n",
    "                        noise: bool = True) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "        '''\n",
    "        预处理数据\n",
    "        1. 处理缺失值\n",
    "        2. 处理非数值\n",
    "        3. 进一步处理\n",
    "\n",
    "        df: pd.DataFrame, 原始数据\n",
    "        missing_value_method: str, 缺失值处理方法, drop/impute\n",
    "        low_card_method: str, 低基数特征处理方法, onehot/label/binary\n",
    "        high_card_mehtod: str, 高基数特征处理方法, frequency/target/hashing\n",
    "        return: Tuple[pd.DataFrame, pd.Series], 预处理后的数据和标签\n",
    "        '''\n",
    "\n",
    "        # 清洗\n",
    "        df.columns = df.columns.str.replace('_', '.', regex=False)\n",
    "\n",
    "        if missing_value_method == \"drop\":\n",
    "            # 处理缺失值->直接删\n",
    "            df.replace(' ?', pd.NA, inplace=True)\n",
    "            df.dropna(inplace=True)\n",
    "        elif missing_value_method == \"impute\":\n",
    "\n",
    "            # 处理缺失值->插补，类别用众数，数值用中位数\n",
    "            numeric_cols = ['age', 'fnlwgt', 'education-num',\n",
    "                            'capital-gain', 'capital-loss', 'hours-per-week']\n",
    "            categorical_cols = ['workclass', 'education', 'marital-status', 'occupation',\n",
    "                                'relationship', 'race', 'sex', 'native-country']\n",
    "\n",
    "            for col in numeric_cols:\n",
    "                df[col] = df[col].fillna(df[col].median())\n",
    "\n",
    "            for col in categorical_cols:\n",
    "                df[col] = df[col].fillna(df[col].mode()[0])\n",
    "\n",
    "        # 处理非数值\n",
    "        low_card = ['sex', 'relationship', 'race', 'marital-status']\n",
    "        high_card = ['workclass', 'education', 'occupation', 'native-country']\n",
    "\n",
    "        if low_card_method == \"onehot\":\n",
    "            # 低基数->onehot\n",
    "            df = pd.get_dummies(df, columns=low_card, drop_first=True)\n",
    "        elif low_card_method == \"label\":\n",
    "            # 低基数->label encoding\n",
    "            for col in low_card:\n",
    "                le = LabelEncoder()\n",
    "                df[col] = le.fit_transform(df[col].astype(str))\n",
    "        elif low_card_method == \"binary\":\n",
    "            # 低基数->Binary encoding\n",
    "            encoder = ce.BinaryEncoder(cols=low_card)\n",
    "            df = encoder.fit_transform(df)\n",
    "        else:\n",
    "            raise ValueError(\"low_card_method must be onehot, label or binary\")\n",
    "\n",
    "        if high_card_mehtod == \"frequency\":\n",
    "            # 高基数->频率编码\n",
    "            for col in high_card:\n",
    "                freq = df[col].value_counts(normalize=True)\n",
    "                df[col] = df[col].map(freq)\n",
    "        elif high_card_mehtod == \"target\":\n",
    "            # 高基数->目标编码\n",
    "            for col in high_card:\n",
    "                # 确保 'income' 列的值被正确处理\n",
    "                df['income'] = df['income'].str.strip()  # 去除多余空格\n",
    "                # 计算目标编码\n",
    "                freq = df.groupby(col)['income'].value_counts(\n",
    "                    normalize=True).unstack().fillna(0)\n",
    "                # 检查列名是否为 '>50K' 或其他值\n",
    "                if '>50K' in freq.columns:\n",
    "                    df[col] = df[col].map(freq['>50K'])\n",
    "                else:\n",
    "                    raise ValueError(\n",
    "                        f\"Unexpected income column names in target encoding: {freq.columns}\")\n",
    "        elif high_card_mehtod == \"hashing\":\n",
    "            # 高基数->Hashing encoding\n",
    "            encoder = ce.HashingEncoder(cols=high_card, n_components=8)\n",
    "            df = encoder.fit_transform(df)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"high_card_mehtod must be frequency or target\")\n",
    "        # 分离标签\n",
    "        y = df['income'].str.strip().map({'>50K': 1, '<=50K': 0})\n",
    "        X = df.drop('income', axis=1)\n",
    "\n",
    "        if if_standard:\n",
    "            # 标准化\n",
    "            scaler = StandardScaler()\n",
    "            X_scaler = scaler.fit_transform(X)\n",
    "            X = pd.DataFrame(X_scaler, columns=X.columns, index=X.index)\n",
    "\n",
    "        if noise:\n",
    "            X = self.data_noise(X, 0.01)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def check_missing_values(self, df: pd.DataFrame):\n",
    "        '''\n",
    "        检测缺失值\n",
    "        df: pd.DataFrame, 原始数据\n",
    "        '''\n",
    "\n",
    "        df.columns = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status',\n",
    "                      'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss',\n",
    "                      'hours-per-week', 'native-country', 'income']\n",
    "\n",
    "        # 替换表示缺失值的字符串\n",
    "        df.replace(' ?', pd.NA, inplace=True)\n",
    "\n",
    "        # 查看缺失值情况\n",
    "        print(\"缺失值统计：\")\n",
    "        print(df.isna().sum())\n",
    "\n",
    "        # 方法1：直接删除缺失值的样本\n",
    "        df_dropna = df.dropna()\n",
    "\n",
    "        # 方法2：针对类别变量使用众数插补，数值变量使用中位数插补（示例）\n",
    "        df_impute = df.copy()\n",
    "        numeric_cols = ['age', 'fnlwgt', 'education-num',\n",
    "                        'capital-gain', 'capital-loss', 'hours-per-week']\n",
    "        categorical_cols = ['workclass', 'education', 'marital-status', 'occupation',\n",
    "                            'relationship', 'race', 'sex', 'native-country', 'income']\n",
    "\n",
    "        for col in numeric_cols:\n",
    "            df_impute[col] = df_impute[col].fillna(df_impute[col].median())\n",
    "\n",
    "        for col in categorical_cols:\n",
    "            df_impute[col] = df_impute[col].fillna(df_impute[col].mode()[0])\n",
    "\n",
    "        # 对比处理结果\n",
    "        print(\"删除缺失值后样本量：\", df_dropna.shape[0])\n",
    "        print(\"插补后样本量：\", df_impute.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2cf048e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluation:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def plot_confusion_matrix(self, y_true, y_pred, title='Confusion Matrix'):\n",
    "        \"\"\"\n",
    "        绘制混淆矩阵，同时显示每个单元格的计数及百分比，并调整字体大小\n",
    "        \"\"\"\n",
    "        import numpy as np\n",
    "        from sklearn.metrics import confusion_matrix\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        total = np.sum(cm)\n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "        im = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "        ax.figure.colorbar(im, ax=ax)\n",
    "\n",
    "        # 设置坐标轴标签和刻度字体大小\n",
    "        ax.set(\n",
    "            xticks=np.arange(cm.shape[1]),\n",
    "            yticks=np.arange(cm.shape[0]),\n",
    "            xlabel='Predicted',\n",
    "            ylabel='True',\n",
    "            title=title\n",
    "        )\n",
    "        ax.tick_params(axis='both', labelsize=14)  # 调整刻度文字大小\n",
    "\n",
    "        # 在每个格子上标注计数和百分比，并设置字体大小\n",
    "        thresh = cm.max() / 2.0\n",
    "        for i in range(cm.shape[0]):\n",
    "            for j in range(cm.shape[1]):\n",
    "                percentage = cm[i, j] / total\n",
    "                ax.text(j, i, f'{cm[i, j]}\\n({percentage:.2%})',\n",
    "                        ha=\"center\", va=\"center\",\n",
    "                        color=\"white\" if cm[i, j] > thresh else \"black\",\n",
    "                        fontsize=26)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_decision_boundary(self, model, x, y, title):\n",
    "        \"\"\"\n",
    "        绘制决策边界\n",
    "        \"\"\"\n",
    "        # 使用 PCA 降维到 2D\n",
    "        pca = PCA(n_components=2, random_state=42)\n",
    "        X2 = pca.fit_transform(x)\n",
    "        x_min, x_max = X2[:, 0].min() - 1, X2[:, 0].max() + 1\n",
    "        y_min, y_max = X2[:, 1].min() - 1, X2[:, 1].max() + 1\n",
    "        xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
    "                             np.linspace(y_min, y_max, 200))\n",
    "        Z = model.predict(pca.inverse_transform(np.c_[xx.ravel(), yy.ravel()]))\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        plt.contourf(xx, yy, Z, alpha=0.3)\n",
    "        plt.scatter(X2[:, 0], X2[:, 1], c=y, edgecolor='k')\n",
    "        plt.title(title)\n",
    "        plt.xlabel(\"PC1\")\n",
    "        plt.ylabel(\"PC2\")\n",
    "        plt.show()\n",
    "\n",
    "    def plot_roc_curve(self, y_true, y_scores, title='ROC Curve'):\n",
    "        \"\"\"\n",
    "        绘制ROC曲线\n",
    "        \"\"\"\n",
    "        # SVM 的得分：decision_function；其他模型可用 predict_proba\n",
    "        fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plt.figure()\n",
    "        plt.plot(fpr, tpr, color='darkorange', lw=2,\n",
    "                 label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title(title)\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.show()\n",
    "\n",
    "    def classification_report(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        打印分类报告\n",
    "        \"\"\"\n",
    "        report = classification_report(y_true, y_pred)\n",
    "        print(report)\n",
    "        return report\n",
    "\n",
    "    def roc_for_three(self, models, x_train, y_train, x_test, y_test):\n",
    "        \"\"\"\n",
    "        将多个模型的 ROC 曲线绘制在同一张图上，\n",
    "        显示每个模型在训练集、测试集和整体数据上的表现\n",
    "\n",
    "        参数:\n",
    "        models: list[tuple], 格式为 [(model_name, model), ...]\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        for name, model in models:\n",
    "            # 判断模型是否支持 predict_proba 或 decision_function\n",
    "            if hasattr(model, \"predict_proba\"):\n",
    "                y_train_scores = model.predict_proba(x_train)[:, 1]\n",
    "                y_test_scores = model.predict_proba(x_test)[:, 1]\n",
    "                y_total_scores = model.predict_proba(\n",
    "                    np.concatenate((x_train, x_test)))[:, 1]\n",
    "            elif hasattr(model, \"decision_function\"):\n",
    "                y_train_scores = model.decision_function(x_train)\n",
    "                y_test_scores = model.decision_function(x_test)\n",
    "                y_total_scores = model.decision_function(\n",
    "                    np.concatenate((x_train, x_test)))\n",
    "            else:\n",
    "                print(f\"模型 {name} 不支持 ROC 曲线绘制\")\n",
    "                continue\n",
    "\n",
    "            # 计算 ROC 曲线和 AUC 分数\n",
    "            fpr_train, tpr_train, _ = roc_curve(y_train, y_train_scores)\n",
    "            roc_auc_train = auc(fpr_train, tpr_train)\n",
    "\n",
    "            fpr_test, tpr_test, _ = roc_curve(y_test, y_test_scores)\n",
    "            roc_auc_test = auc(fpr_test, tpr_test)\n",
    "\n",
    "            fpr_total, tpr_total, _ = roc_curve(\n",
    "                np.concatenate((y_train, y_test)), y_total_scores)\n",
    "            roc_auc_total = auc(fpr_total, tpr_total)\n",
    "\n",
    "            # 绘制三条曲线：训练集 (实线)，测试集 (虚线)，整体 (点线)\n",
    "            plt.plot(fpr_train, tpr_train, lw=2, linestyle='-',\n",
    "                     label=f'{name} - Train (AUC = {roc_auc_train:.2f})')\n",
    "            plt.plot(fpr_test, tpr_test, lw=2, linestyle='--',\n",
    "                     label=f'{name} - Test (AUC = {roc_auc_test:.2f})')\n",
    "            plt.plot(fpr_total, tpr_total, lw=2, linestyle=':',\n",
    "                     label=f'{name} - Total (AUC = {roc_auc_total:.2f})')\n",
    "\n",
    "        # 绘制随机猜测的对角线\n",
    "        plt.plot([0, 1], [0, 1], color='grey', lw=2, linestyle='--')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate', fontsize=14)\n",
    "        plt.ylabel('True Positive Rate', fontsize=14)\n",
    "        plt.title('Multiple Models ROC Curve', fontsize=16)\n",
    "        plt.legend(loc=\"lower right\", fontsize=12)\n",
    "        plt.show()\n",
    "\n",
    "    def evaluate_model(self, model, x_train, y_train, x_test, y_test):\n",
    "        \"\"\"\n",
    "        评估模型性能\n",
    "        \"\"\"\n",
    "        y_train_pred = model.predict(x_train)\n",
    "        y_test_pred = model.predict(x_test)\n",
    "\n",
    "        # 计算准确率\n",
    "        train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "        test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "        print(f\"Train Accuracy: {train_accuracy:.4f}\")\n",
    "        print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "        # 绘制混淆矩阵\n",
    "        # self.plot_confusion_matrix(\n",
    "        #     y_train, y_train_pred, title='Train Confusion Matrix')\n",
    "        # self.plot_confusion_matrix(\n",
    "        #     y_test, y_test_pred, title='Test Confusion Matrix')\n",
    "        # # 绘制总体混淆矩阵\n",
    "        # self.plot_confusion_matrix(\n",
    "        #     np.concatenate((y_train, y_test)), np.concatenate((y_train_pred, y_test_pred)), title='Total Confusion Matrix')\n",
    "\n",
    "        # # 绘制ROC曲线\n",
    "        # # 这里假设模型有 predict_proba 方法\n",
    "        # if hasattr(model, \"predict_proba\"):\n",
    "        #     y_train_scores = model.predict_proba(x_train)[:, 1]\n",
    "        #     y_test_scores = model.predict_proba(x_test)[:, 1]\n",
    "        #     self.plot_roc_curve(y_train, y_train_scores,\n",
    "        #                         title='Train ROC Curve')\n",
    "        #     self.plot_roc_curve(y_test, y_test_scores, title='Test ROC Curve')\n",
    "        #     # 绘制总体ROC曲线\n",
    "        #     y_total_scores = model.predict_proba(\n",
    "        #         np.concatenate((x_train, x_test)))[:, 1]\n",
    "        #     self.plot_roc_curve(np.concatenate((y_train, y_test)),\n",
    "        #                         y_total_scores, title='Total ROC Curve')\n",
    "        # # 如果模型是 SVM，使用 decision_function\n",
    "        # elif hasattr(model, \"decision_function\"):\n",
    "        #     y_train_scores = model.decision_function(x_train)\n",
    "        #     y_test_scores = model.decision_function(x_test)\n",
    "        #     self.plot_roc_curve(y_train, y_train_scores,\n",
    "        #                         title='Train ROC Curve')\n",
    "        #     self.plot_roc_curve(y_test, y_test_scores, title='Test ROC Curve')\n",
    "        #     # 绘制总体ROC曲线\n",
    "        #     y_total_scores = model.decision_function(\n",
    "        #         np.concatenate((x_train, x_test)))\n",
    "        #     self.plot_roc_curve(np.concatenate((y_train, y_test)),\n",
    "        #                         y_total_scores, title='Total ROC Curve')\n",
    "\n",
    "        # 打印分类报告\n",
    "        print(\"Classification Report:\")\n",
    "        print(\"Train Set:\")\n",
    "        self.classification_report(y_train, y_train_pred)\n",
    "        print(\"Test Set:\")\n",
    "        self.classification_report(y_test, y_test_pred)\n",
    "        print(\"Total Set:\")\n",
    "        self.classification_report(\n",
    "            np.concatenate((y_train, y_test)), np.concatenate((y_train_pred, y_test_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd62f01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Classifier:\n",
    "    def __init__(self, model, random_state=42):\n",
    "        self.dataloader = DataLoader()\n",
    "        self.model = model\n",
    "        self.random_state = random_state\n",
    "        self.results = None\n",
    "\n",
    "    def set_model(self, model):\n",
    "        \"\"\"\n",
    "        设置模型\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "\n",
    "    def load_and_preprocess_data(self, file_path, test_size=0.3):\n",
    "        \"\"\"\n",
    "        加载并预处理数据\n",
    "        \"\"\"\n",
    "        raw_data = self.dataloader.load_data(file_path)\n",
    "        self.x, self.y = self.dataloader.preprocess_data(raw_data)\n",
    "        self.train_x, self.test_x, self.train_y, self.test_y = train_test_split(\n",
    "            self.x, self.y, test_size=test_size, random_state=self.random_state\n",
    "        )\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        训练 SVM 模型\n",
    "        \"\"\"\n",
    "        self.model.fit(self.train_x, self.train_y)\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        预测\n",
    "        \"\"\"\n",
    "        return self.model.predict(x)\n",
    "\n",
    "    def evaluate(self):\n",
    "        \"\"\"\n",
    "        评估模型性能\n",
    "        \"\"\"\n",
    "        self.train_y_pred = self.model.predict(self.train_x)\n",
    "        self.test_y_pred = self.model.predict(self.test_x)\n",
    "        self.total_y_pred = self.model.predict(self.x)\n",
    "\n",
    "        print(\"accuracy on train set: \", accuracy_score(\n",
    "            self.train_y, self.train_y_pred))\n",
    "        print(\"accuracy on test set: \", accuracy_score(\n",
    "            self.test_y, self.test_y_pred))\n",
    "        print(\"accuracy on total set: \",\n",
    "              accuracy_score(self.y, self.total_y_pred))\n",
    "\n",
    "    def random_search(self, param_grid, n_iter=5):\n",
    "        \"\"\"\n",
    "        随机搜索超参数\n",
    "        \"\"\"\n",
    "        # 定义随机搜索\n",
    "        search = RandomizedSearchCV(\n",
    "            self.model,\n",
    "            param_distributions=param_grid,\n",
    "            n_iter=n_iter,\n",
    "            scoring='roc_auc',\n",
    "            cv=2,\n",
    "            random_state=self.random_state,\n",
    "            n_jobs=-1,\n",
    "            verbose=2,\n",
    "        )\n",
    "        search.fit(self.train_x, self.train_y)\n",
    "        print(\"Best parameters found: \", search.best_params_)\n",
    "        return search.best_estimator_, search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c97ae61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    # (\"Logistic Regression\", LogisticRegression(\n",
    "    #     max_iter=1000,\n",
    "    #     random_state=42,\n",
    "    #     C=1,\n",
    "    #     penalty='l2',\n",
    "    #     solver='liblinear',\n",
    "    # )),\n",
    "    # (\"Decision Tree\", DecisionTreeClassifier(\n",
    "    #     criterion='gini',\n",
    "    #     min_samples_split=20,\n",
    "    #     min_samples_leaf=5,\n",
    "    #     max_depth=10,\n",
    "    #     random_state=42\n",
    "    # )),\n",
    "    # (\"SVM\", SVC(\n",
    "    #     kernel='rbf',\n",
    "    #     random_state=42,\n",
    "    #     gamma='scale',\n",
    "    #     probability=True,\n",
    "    # )),\n",
    "    (\"Random Forest\", RandomForestClassifier(\n",
    "        n_estimators=100, max_depth=6, random_state=42)),\n",
    "    (\"XGBoost\", XGBClassifier(\n",
    "        objective='binary:logistic',\n",
    "        eval_metric='logloss',\n",
    "        eta=0.1,\n",
    "        max_depth=6,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42\n",
    "    )),\n",
    "    (\"KNN\", KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=2)),\n",
    "    (\"MLP\", MLPClassifier(\n",
    "        hidden_layer_sizes=(100,),  # 隐藏层大小（100 个神经元）\n",
    "        activation='relu',         # 激活函数（默认 relu）\n",
    "        solver='adam',             # 优化器（默认 adam）\n",
    "        max_iter=300,              # 最大迭代次数\n",
    "        random_state=42\n",
    "    )),\n",
    "    (\"LGBM\", LGBMClassifier(\n",
    "        objective='binary',\n",
    "        metric='binary_logloss',\n",
    "        learning_rate=0.1,\n",
    "        max_depth=6,\n",
    "        num_leaves=31,\n",
    "        min_child_samples=10,\n",
    "        min_split_gain=0.01,\n",
    "        max_bin=128,\n",
    "        class_weight='balanced',\n",
    "        random_state=42\n",
    "    )),\n",
    "    (\"CatBoost\", CatBoostClassifier(\n",
    "        iterations=1000,\n",
    "        depth=6,\n",
    "\n",
    "        learning_rate=0.1,\n",
    "        loss_function='Logloss',\n",
    "        random_seed=42,\n",
    "        verbose=0\n",
    "    ))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfa7e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_train_test():\n",
    "    evaluation = Evaluation()\n",
    "    # 使用第一个模型初始化 Classifier\n",
    "    classifier = Classifier(model=models[0][1], random_state=42)\n",
    "    classifier.load_and_preprocess_data('raw')\n",
    "    trained_models = []\n",
    "\n",
    "    # 遍历模型列表，依次训练、评估\n",
    "    for title, model in models:\n",
    "        print(f\"=== {title} Evaluation ===\")\n",
    "        classifier.set_model(model)\n",
    "        start_time = time.time()\n",
    "        classifier.train()\n",
    "        end_time = time.time()\n",
    "        trained_models.append((title, classifier.model))\n",
    "        evaluation.evaluate_model(\n",
    "            classifier.model,\n",
    "            classifier.train_x,\n",
    "            classifier.train_y,\n",
    "            classifier.test_x,\n",
    "            classifier.test_y\n",
    "        )\n",
    "        # evaluation.plot_decision_boundary(\n",
    "        #     classifier.model,\n",
    "        #     classifier.x,\n",
    "        #     classifier.y,\n",
    "        #     title=f\"{title} Decision Boundary (Total Set)\"\n",
    "        # )\n",
    "        print(f\"Training time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "        # print(\"=== Cross Validation ===\")\n",
    "        # start_time = time.time()\n",
    "        # try:\n",
    "        #     # 确保模型兼容 cross_val_score\n",
    "        #     if hasattr(model, \"fit\") and hasattr(model, \"predict\"):\n",
    "        #         scores = cross_val_score(\n",
    "        #             classifier.model, classifier.x, classifier.y, cv=5\n",
    "        #         )\n",
    "        #         print(f\"Cross-validation scores: {scores}\")\n",
    "        #         print(f\"Mean cross-validation score: {scores.mean():.4f}\")\n",
    "        #     else:\n",
    "        #         print(f\"{title} is not compatible with cross_val_score.\")\n",
    "        # except Exception as e:\n",
    "        #     print(f\"Error during cross-validation for {title}: {e}\")\n",
    "        # end_time = time.time()\n",
    "        # print(f\"Cross-validation time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "    # evaluation.roc_for_three(\n",
    "    #     trained_models,\n",
    "    #     classifier.train_x,\n",
    "    #     classifier.train_y,\n",
    "    #     classifier.test_x,\n",
    "    #     classifier.test_y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
